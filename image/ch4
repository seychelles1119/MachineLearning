- 여기서는 가장 간단한 모델 중 하나인 선형 회귀부터 시작. 이 모델을 훈련시키는 두 가지 방법
- * 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터(즉, 훈련세트에 대해 비용 함수를 최소화하는 모델 파라미터)를 해석적으로 구한다.
- * 경사하강법(Gradient Descent)이라 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련세트에 대해 최소화시킴
- * * 결국은 앞서의 방법과 동일한 파라미터로 수렴함. 경사하강법의 변종?으로 Batch경사하강법, Mini-Batch경사하강법, 확률적경사하강법에 대해서도 살펴볼 예정
- 그 다음에는 비선형 데이터셋에 훈련시킬 수 있는 조금 더 복잡한 모델인 다항 회귀를 살펴볼 예정임.
- * 이 모델은 선형 회귀보다 파라미터가 많아서 훈련데이터에 과대적합되기 더 쉬움
- * 따라서, 학습 곡선을 사용해 모델이 과대적합되는지 감지하는 방법도 살펴볼 것임
- * 그런 다음, 훈련 세트의 과대적합을 감소시킬 수 있는 규제기법 몇가지에 대해서도 알아볼 예정임.
- 마지막으로 분류작업에 널리 사용하는 모델인 로지스틱 회귀와 소프트맥스 회귀를 살펴볼 예정.
4.1 선형 회귀
- 일반적으로 선형 모델은 입력 특성(feature)의 가중치 합과 편향(bias, intercept)이라는 상수를 더해 예측을 만듬
- 선형회귀모델의 예측 --> 벡터 형태로 표현
- learning model
- * 모델을 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것임. 이를 위해 먼저 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정해야 함. 여기서는 평균제곱오차(MSE, Mean Squared Error)를 최소화하는 theta를 찾음.
- * training set X에 대한 선형회귀가설 h의 MSE는 다음과 같이 계산
- * * 
